import pymc as pm
import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import odeint
import arviz as az
import sunode.wrappers.as_pytensor


def SIR(y, t, p):
    ds = -p[0] * y[0] * y[1]
    di = p[0] * y[0] * y[1] - p[1] * y[1]
    return [ds, di]


times = np.arange(0, 5, 0.25)

beta, gamma = 4, 1.0
# Create true curves
y = odeint(SIR, t=times, y0=[0.99, 0.01], args=((beta, gamma),), rtol=1e-8)
# Observational model.  Lognormal likelihood isn't appropriate, but we'll do it anyway
yobs = np.random.lognormal(mean=np.log(y[1::]), sigma=[0.2, 0.3])

plt.plot(times[1::], yobs, marker="o", linestyle="none")
plt.plot(times, y[:, 0], color="C0", alpha=0.5, label=f"$S(t)$")
plt.plot(times, y[:, 1], color="C1", alpha=0.5, label=f"$I(t)$")
plt.legend()
plt.show()


sir_model = pm.ode.DifferentialEquation(
    func=SIR,
    times=np.arange(0.25, 5, 0.25),
    n_states=2,
    n_theta=2,
    t0=0,
)

with pm.Model() as model4:
    sigma = pm.HalfCauchy("sigma", 1, shape=2)

    # R0 is bounded below by 1 because we see an epidemic has occurred
    R0 = pm.Normal("R0", 2, 3)
    lam = pm.Lognormal("lambda", pm.math.log(2), 2)
    beta = pm.Deterministic("beta", lam * R0)

    sir_curves = sir_model(y0=[0.99, 0.01], theta=[beta, lam])

    Y = pm.Lognormal("Y", mu=pm.math.log(sir_curves), sigma=sigma, observed=yobs)

    trace = pm.sample(2000, tune=1000, target_accept=0.9, cores=4)
    


az.plot_trace(trace)
plt.show()


ppc = pm.sample_posterior_predictive(trace, var_names=['Y'], model=model4)


az.plot_ppc(ppc)
plt.scatter(np.arange(0.25, 5, 0.25), yobs[:, 0], c='blue', label='Observed S')
plt.scatter(np.arange(0.25, 5, 0.25), yobs[:, 1], c='red', label='Observed I')
plt.legend()
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('Posterior Predictive Checks')
plt.show()




def lotka_volterra(t, y, p):
    """Right hand side of Lotka-Volterra equation.

    All inputs are dataclasses of sympy variables, or in the case
    of non-scalar variables numpy arrays of sympy variables.
    """
    return {
        'S': p.alpha * (1-y.S-y.I) - p.R_0*p.gamma * y.S * y.I,
        'I': p.R_0*p.gamma * y.S * y.I - p.gamma * y.I,
    }


with pm.Model() as model:
    
    # Compute the parameters of the ode based on our prior parameters
    sigma = pm.HalfCauchy("sigma", 1,shape=2)

    # R0 is bounded below by 1 because we see an epidemic has occurred
    
    R_0 = pm.LogNormal("R_0", 2, 3)
    lam = pm.LogNormal("lambda", 1, 2)
    beta = pm.Deterministic("beta", lam * R_0)
    gamma = pm.LogNormal('gamma', mu=1, sigma=0.1)
    S_start=0.99 
    I_start=0.01
    alpha=0 
    extra=0
    y_hat, _, problem, solver, _, _ = sunode.wrappers.as_pytensor.solve_ivp(
        y0={
            'S': (S_start, ()),  # Ensure shape is ()
            'I': (I_start, ()),  # Ensure shape is ()
        },
        params={
            'alpha': (alpha, ()),
            'R_0': (R_0, ()),  # Ensure shape is (100,)
            'beta': (beta, ()),
            'gamma': (gamma, ()),
            'extra': (extra, ()),
        },
        rhs=lotka_volterra,
        tvals=times,
        t0=times[0],
    )
        
    
# We can access the individual variables of the solution using the
# variable names.
    pm.Deterministic('S_mu', pm.math.clip(y_hat['S'], 0, 1))
    pm.Deterministic('I_mu', pm.math.clip(y_hat['I'], 0, 1))
    S = pm.Normal("S", mu=y_hat['S'], sigma=sigma[0], observed=y[:, 0])
    I = pm.Normal("I", mu=y_hat['I'], sigma=sigma[1], observed=y[:, 1])
    
    trace = pm.sample(1000, tune=1000, cores=2, return_inferencedata=True)

    



ppc = pm.sample_posterior_predictive(trace, var_names=['S','I'], model=model)
ppc
#plt.scatter(np.arange(0.25, 5, 0.25), yobs[:, 0], c='blue', label='Observed S')
#plt.scatter(np.arange(0.25, 5, 0.25), yobs[:, 1], c='red', label='Observed I')



ppc_I=ppc.posterior_predictive.I


plt.figure(figsize=(10, 6))
for sample in ppc_I[0,:,:]:
    plt.plot(times, sample, color='gray', alpha=0.5)



summary = az.summary(trace)
print(summary)





times


az.plot_trace(trace)





plt.scatter(times[1:], yobs[:, 0], c='blue', label='Observed S')
plt.scatter(times[1:], yobs[:, 1], c='red', label='Observed I')
plt.legend()
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('Posterior Predictive Checks')
plt.show()


import numpy as np
import sunode
import sunode.wrappers.as_pytensor
import pymc as pm
import matplotlib.pyplot as plt
import sympy as sp

# Time points
t = np.linspace(0, 100, 100)
times = np.arange(0, 5, 0.25)
# Observed R values (example data)
R_obs = np.array([0.78843596, 0.71150383, 0.66289627, 0.64019564, 0.63681664,
                  0.6445331, 0.66119279, 0.68034687, 0.6983999, 0.71331924,
                  0.72873873, 0.73886516, 0.7501649, 0.76678487, 0.77994872,
                  0.7942952, 0.80841032, 0.82774465, 0.84001028, 0.84352969,
                  0.84642354, 0.84616422, 0.84338121, 0.83374183, 0.82259294,
                  0.81345853, 0.80554934, 0.79795632, 0.78799997, 0.78669823,
                  0.79520117, 0.80754271, 0.8283006, 0.85634427, 0.89385511,
                  0.93620753, 0.97208123, 1.00983813, 1.03529901, 1.05014702,
                  1.06611123, 1.08140734, 1.09290491, 1.09983065, 1.09641181,
                  1.11140258, 1.12982013, 1.14538746, 1.16677504, 1.1804819,
                  1.18267914, 1.18360481, 1.16307675, 1.13732533, 1.12099681,
                  1.0982526, 1.07766902, 1.06453642, 1.05820409, 1.04992038,
                  1.04277409, 1.0397242, 1.04208068, 1.04451385, 1.05210841,
                  1.06788159, 1.07745923, 1.07809487, 1.06370824, 1.04013859,
                  1.01094897, 0.97686433, 0.93662186, 0.89594093, 0.8590])

def lotka_volterra(t, y, p):
    """Right hand side of Lotka-Volterra equation.

    All inputs are dataclasses of sympy variables, or in the case
    of non-scalar variables numpy arrays of sympy variables.
    """
    return {
        'S': p.alpha * (1-y.S-y.I) - p.R_0*p.gamma * y.S * y.I,
        'I': p.R_0*p.gamma * y.S * y.I - p.gamma * y.I,
    }


with pm.Model() as model:
    S_start = pm.HalfNormal('S_start', sigma=0.1)
    I_start = pm.HalfNormal('I_start', sigma=0.1)
    
    
    
    # Compute the parameters of the ode based on our prior parameters
    alpha = 0. #pm.Deterministic('alpha', np.array(0))
    beta = pm.Normal('beta', mu=0.1, sigma=0.01)
    gamma = pm.Normal('gamma', mu=0.1, sigma=0.01)
    R_0 = pm.Normal('R_0', mu=1, sigma=0.1)#, observed=R_obs)
    
    
    y_hat, _, problem, solver, _, _ = sunode.wrappers.as_pytensor.solve_ivp(
        y0={
            'S': (S_start, ()),  # Ensure shape is ()
            'I': (I_start, ()),  # Ensure shape is ()
        },
        params={
            'alpha': (alpha, ()),
            'R_0': (R_0, ()),  # Ensure shape is (100,)
            'beta': (beta, ()),
            'gamma': (gamma, ()),
        },
        rhs=lotka_volterra,
        tvals=times[1:],
        t0=times[1],
    )
        
    
    # We can access the individual variables of the solution using the
    # variable names.
    pm.Deterministic('S_mu', y_hat['S'])
    pm.Deterministic('I_mu', y_hat['I'])

    sd = pm.HalfNormal('sd')
    S = pm.LogNormal('S', mu=np.log(y_hat['S']), sigma=sd, observed=yobs[:, 0])
    I = pm.LogNormal('I', mu=np.log(y_hat['I']), sigma=sd, observed=yobs[:, 1])
    
    trace = pm.sample(1000, tune=1000, cores=2, return_inferencedata=True)

    



az.plot_ppc(trace)
plt.scatter(times[1:], yobs[:, 0], c='blue', label='Observed S')
plt.scatter(times[1:], yobs[:, 1], c='red', label='Observed I')
plt.legend()
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('Posterior Predictive Checks')
plt.show()




az.plot_trace(trace)
plt.subplots_adjust(hspace=0.5)  # Increase the value to 
plt.show()



ppc = pm.sample_posterior_predictive(trace, var_names=['S','I'], model=model)


ppc


az.plot_ppc(ppc)
plt.scatter(np.arange(0.25, 5, 0.25), yobs[:, 0], c='blue', label='Observed S')
plt.scatter(np.arange(0.25, 5, 0.25), yobs[:, 1], c='red', label='Observed I')
plt.legend()
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('Posterior Predictive Checks')
plt.show()


import pandas as pd

# Read the CSV file into a DataFrame
cases_df = pd.read_csv('cases.csv')

# Display the first few rows of the DataFrame
cases_df.head()





import pymc as pm
import numpy as np

def seir_difference(S, E, I, R, beta, sigma, gamma):
    new_exposed = beta * S * I
    new_infected = sigma * E
    new_recovered = gamma * I

    S_next = S - new_exposed
    E_next = E + new_exposed - new_infected
    I_next = I + new_infected - new_recovered
    R_next = R + new_recovered

    return S_next, E_next, I_next, R_next, new_infected

# Observed data (replace with your actual data)
observed_new_cases = (cases_df.n/5000000).values

with pm.Model() as model:
    # Priors for parameters
    beta = pm.HalfNormal('beta', sigma=1)
    sigma = pm.HalfNormal('sigma', sigma=1)
    gamma = pm.HalfNormal('gamma', sigma=1)

    # Initial conditions
    S0 = pm.Beta('S0', alpha=2, beta=2)
    E0 = pm.Beta('E0', alpha=2, beta=2)
    I0 = pm.Beta('I0', alpha=2, beta=2)
    R0 = pm.Deterministic('R0', 1 - S0 - E0 - I0)
    

    # Ensure initial conditions sum to 1
    pm.Potential('sum_to_one', pm.math.eq(S0 + E0 + I0 + R0, 1))

    # Time points
    T = len(observed_new_cases)

    # Initialize states
    S = S0
    E = E0
    I = I0
    R = R0

    # Lists to store results
    new_cases = []

    for t in range(T):
        S, E, I, R, new_infected = seir_difference(S, E, I, R, beta, sigma, gamma)
        new_cases.append(new_infected.astype('float64'))

    # Convert list to NumPy array
    new_cases = np.array(new_cases)

    # Observations
    new_cases_det = pm.Deterministic('new_cases', new_cases)
    I_obs = pm.Normal('I_obs', mu=new_cases_det, sigma=0.1, observed=observed_new_cases)



observed_new_cases.plot()




